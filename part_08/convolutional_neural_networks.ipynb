{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "<b>Key Layers in a CNN: </b> <br>\n",
    "\n",
    "<small>\n",
    "\n",
    "1. <b>Convolutional Layer:</b> Applies a filter to the input image to produce feature maps, capturing spatial relationships in the image\n",
    "\n",
    "    - Feature Extraction: Convolution helps the network find important features like edges, shapes, or textures in the image, without needing to tell it exactly what to look for.\n",
    "\n",
    "    - Local Connectivity: Instead of looking at the entire image at once, the filter focuses on small parts (local patterns), which makes it more efficient and able to detect detailed features.\n",
    "\n",
    "        <br>\n",
    "\n",
    "- <b>ReLU Activation:</b> Is applied element-wise to the feature map produced by the convolutional layer. Introduces non-linearity by setting all negative values to zero, allowing the network to learn complex patterns.\n",
    "\n",
    "    <br>\n",
    "\n",
    "Together, they form the basic building block in CNNs, where the <b>convolutional layer extracts features</b>, and the <b>ReLU activation enables non-linear decision boundaries</b>.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "2. <b>Pooling Layer:</b> Reduces the spatial dimensions of the feature maps, retaining essential information while reducing computation, prevents overfitting, and translating invariance\n",
    "    - Max/ Min/ Average/ Sum pooling: take the max/ min/ average/ sum value in a region\n",
    "\n",
    "        <br>\n",
    "\n",
    "\n",
    "3. <b>Flattening Layer:</b> Converts the 2D (or higher-dimensional) feature maps into a 1D array (vector) so that they can be fed into the fully connected layer\n",
    "\n",
    "    <br>\n",
    "\n",
    "\n",
    "4. <b>Fully Connected Layer: </b> Combines the features from the previous layers to make final predictions\n",
    "\n",
    "    <br>\n",
    "\n",
    "<b>Finally, in Output</b> layer <b>Softmax Cross-Entropy</b> is just a fancy way to:\n",
    "\n",
    "    1. Turn the raw scores (logits) into probabilities. Then pick the highest one to make final decision (softmax)\n",
    "\n",
    "    2. Check how wrong the prediction is using penalties (cross-entropy)\n",
    "    \n",
    "    3. Help the box learn and imporve!üéØ\n",
    "\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the Training set\n",
    "- some transformation to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,        # feature scaling --> Normalization (0, 1)\n",
    "    shear_range=0.2,       # Apply shear transformations\n",
    "    zoom_range=0.2,        # Random zoom\n",
    "    horizontal_flip=True  # Randomly flip images horizontally\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "    'dataset/training_set/',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'dataset/test_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Convolution <br>\n",
    "\n",
    "<small>\n",
    "\n",
    "- filters=32 --> the number of filters (or kernels) the convolutional layer will use\n",
    "\n",
    "- kernel_size=3 --> the filter is a 3x3 matrix\n",
    "\n",
    "- input_shape=[64, 64, 3] --> height and width 64 pixels, and 3 channels (RGB, for color images)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "You can be confident that 32 filters cover the whole image because:\n",
    "\n",
    "    1. Every filter scans the entire image systematically\n",
    "\n",
    "    2. Filters are applied one by one to all parts of the image\n",
    "\n",
    "    3. They work together, producing 32 feature maps that represent different aspects of the image\n",
    "\n",
    "So, the whole image is always covered‚Äîwhat changes is what features are learned by the filters.\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_size=n --> n X n\n",
    "# strides=n --> step size n and skip overlapping\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the CNN on the Training set and evaluating it on the Test set <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 557ms/step - accuracy: 0.9069 - loss: 0.2404 - val_accuracy: 0.8030 - val_loss: 0.5345\n",
      "Epoch 2/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 104ms/step - accuracy: 0.9063 - loss: 0.2247 - val_accuracy: 0.7880 - val_loss: 0.6029\n",
      "Epoch 3/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 98ms/step - accuracy: 0.9180 - loss: 0.2017 - val_accuracy: 0.7890 - val_loss: 0.5740\n",
      "Epoch 4/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 96ms/step - accuracy: 0.9213 - loss: 0.1986 - val_accuracy: 0.7875 - val_loss: 0.6194\n",
      "Epoch 5/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 93ms/step - accuracy: 0.9251 - loss: 0.1867 - val_accuracy: 0.7810 - val_loss: 0.6034\n",
      "Epoch 6/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 95ms/step - accuracy: 0.9301 - loss: 0.1856 - val_accuracy: 0.7960 - val_loss: 0.6066\n",
      "Epoch 7/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 99ms/step - accuracy: 0.9229 - loss: 0.1798 - val_accuracy: 0.7940 - val_loss: 0.6427\n",
      "Epoch 8/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 96ms/step - accuracy: 0.9347 - loss: 0.1663 - val_accuracy: 0.7990 - val_loss: 0.6178\n",
      "Epoch 9/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 97ms/step - accuracy: 0.9354 - loss: 0.1561 - val_accuracy: 0.8005 - val_loss: 0.6699\n",
      "Epoch 10/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 99ms/step - accuracy: 0.9375 - loss: 0.1608 - val_accuracy: 0.7970 - val_loss: 0.6866\n",
      "Epoch 11/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 97ms/step - accuracy: 0.9364 - loss: 0.1625 - val_accuracy: 0.7890 - val_loss: 0.7556\n",
      "Epoch 12/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 98ms/step - accuracy: 0.9370 - loss: 0.1640 - val_accuracy: 0.7825 - val_loss: 0.7028\n",
      "Epoch 13/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 96ms/step - accuracy: 0.9527 - loss: 0.1299 - val_accuracy: 0.7845 - val_loss: 0.7396\n",
      "Epoch 14/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 95ms/step - accuracy: 0.9510 - loss: 0.1257 - val_accuracy: 0.7950 - val_loss: 0.7274\n",
      "Epoch 15/15\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 98ms/step - accuracy: 0.9555 - loss: 0.1212 - val_accuracy: 0.7895 - val_loss: 0.7066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2412ad4c800>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data=test_set, epochs=15) # epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# load_img --> channels=3\n",
    "test_image = image.load_img('dataset/cat.png', target_size=(64, 64)) # after this step --> (64, 64, 3)\n",
    "\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# (batch_size, height, width, channels)\n",
    "# expand_dims --> batch_size=1\n",
    "test_image = np.expand_dims(test_image, axis=0) # axis=0 --> at the beginning\n",
    "\n",
    "test_image = test_image / 255.0\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "training_set.class_indices\n",
    "print(training_set.class_indices)\n",
    "\n",
    "if result[0][0] > 0.5:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "    \n",
    "\n",
    "# if round(result[0][0]) == 1:\n",
    "#     prediction = 'dog'\n",
    "# else:\n",
    "#     prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
